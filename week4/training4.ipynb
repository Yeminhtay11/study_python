{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Neural Networks Overview:\n",
    "\n",
    "    Meaning and Usage: Neural networks are computational models inspired by the structure and function of the human brain. They consist of interconnected layers of nodes (neurons) that process information and learn patterns from data.\n",
    "    Importance: Neural networks are powerful tools for solving complex problems in various domains such as image recognition, natural language processing, and robotics.\n",
    "    Key Concepts: Layers (input, hidden, output), connections (weights), activation functions, feedforward and backpropagation.\n",
    "\n",
    "Perceptron:\n",
    "\n",
    "    Meaning and Usage: A perceptron is the simplest form of a neural network, consisting of a single neuron with multiple input connections, each associated with a weight. It computes a weighted sum of its inputs, applies an activation function, and produces an output.\n",
    "    Importance: Perceptrons serve as the basic building blocks for more complex neural networks. They are used for linear binary classification tasks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Output: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_inputs, activation_function):\n",
    "        self.weights = np.random.rand(num_inputs)\n",
    "        self.bias = np.random.rand()\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.activation_function(weighted_sum)\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "perceptron = Perceptron(num_inputs=2, activation_function=step_function)\n",
    "inputs = np.array([1, 0.5])\n",
    "output = perceptron.forward(inputs)\n",
    "print(\"Perceptron Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Activation Functions:\n",
    "\n",
    "    Meaning and Usage: Activation functions introduce non-linearity to neural networks, enabling them to learn complex patterns. They determine the output of a neuron given its input.\n",
    "    Importance: Activation functions help neural networks model complex relationships in data and avoid the problem of linearity.\n",
    "    Common Activation Functions: Sigmoid, Tanh, ReLU (Rectified Linear Unit), Softmax.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Output: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "ReLU Output: [0 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"Sigmoid Output:\", sigmoid(x))\n",
    "print(\"ReLU Output:\", relu(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ntroduction to TensorFlow/Keras:\n",
    "\n",
    "    Meaning and Usage: TensorFlow is an open-source machine learning library developed by Google Brain. Keras is a high-level neural networks API that runs on top of TensorFlow (or other backend engines) and simplifies the process of building and training neural networks.\n",
    "    Importance: TensorFlow/Keras provide a user-friendly interface for building and training neural networks, making deep learning accessible to beginners and experts alike.\n",
    "    Key Concepts: Tensors, layers, models, loss functions, optimizers.\n",
    "\n",
    "Sequential Models:\n",
    "\n",
    "    Meaning and Usage: Sequential models in Keras allow you to build neural networks layer by layer in a sequential manner. Each layer performs specific operations on the input data and passes the output to the next layer.\n",
    "    Importance: Sequential models provide a simple and intuitive way to create various neural network architectures, including feedforward networks, convolutional networks, and recurrent networks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize sequential model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Initialize sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer (Dense layer)\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(input_dim,)))\n",
    "\n",
    "# Add hidden layers (Dense layers)\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))  # Apply dropout for regularization\n",
    "\n",
    "# Add output layer (Dense layer)\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Meaning and Usage: After building the model, you need to compile it by specifying the loss function, optimizer, and evaluation metrics. Then, you can train the model using the fit method by providing training data and labels.\n",
    "Importance: Compilation and training are essential steps in the deep learning workflow, where the model learns patterns from the training data and adjusts its parameters to minimize the loss.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Introduction to Convolutional Neural Networks (CNNs):\n",
    "\n",
    "    Meaning and Usage: Convolutional Neural Networks (CNNs) are specialized deep learning models designed to process grid-like data such as images. They consist of convolutional layers, pooling layers, and fully connected layers.\n",
    "    Importance: CNNs have revolutionized image recognition tasks by automatically learning hierarchical features from raw pixel values, leading to state-of-the-art performance in various computer vision tasks.\n",
    "    Key Concepts: Convolution, filters/kernels, feature maps, pooling, stride, padding.\n",
    "\n",
    "Building CNNs with TensorFlow/Keras:\n",
    "\n",
    "    Meaning and Usage: TensorFlow/Keras provides high-level APIs for building CNNs with ease. You can stack convolutional layers, pooling layers, and fully connected layers to create a CNN architecture.\n",
    "    Importance: Building CNNs with TensorFlow/Keras allows you to leverage pre-trained models, fine-tune them for specific tasks, or create custom architectures tailored to your needs.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Initialize sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, img_channels)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training CNNs and Transfer Learning:\\n\\n    Meaning and Usage: Training CNNs involves feeding labeled image data into the model and optimizing its parameters using backpropagation. Transfer learning is a technique where you leverage pre-trained CNN models and fine-tune them on your dataset.\\n    Importance: Training CNNs from scratch requires large amounts of labeled data and computational resources. Transfer learning allows you to leverage knowledge learned from one task/domain and apply it to another, often with fewer training samples.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Training CNNs and Transfer Learning:\n",
    "\n",
    "    Meaning and Usage: Training CNNs involves feeding labeled image data into the model and optimizing its parameters using backpropagation. Transfer learning is a technique where you leverage pre-trained CNN models and fine-tune them on your dataset.\n",
    "    Importance: Training CNNs from scratch requires large amounts of labeled data and computational resources. Transfer learning allows you to leverage knowledge learned from one task/domain and apply it to another, often with fewer training samples.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VGG16\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load pre-trained VGG16 model (without top layers)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m base_model \u001b[38;5;241m=\u001b[39m VGG16(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(img_height, img_width, img_channels))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load pre-trained VGG16 model (without top layers)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, img_channels))\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add top layers for classification\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Introduction to Recurrent Neural Networks (RNNs):\n",
    "\n",
    "    Meaning and Usage: Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data, where the output at each time step depends not only on the current input but also on previous inputs in the sequence.\n",
    "    Importance: RNNs are widely used in natural language processing (NLP), time series analysis, and sequence prediction tasks such as language modeling, machine translation, and sentiment analysis.\n",
    "    Key Concepts: Recurrent connections, hidden states, time unfolding, vanishing gradients, long short-term memory (LSTM), gated recurrent unit (GRU).\n",
    "\n",
    "Building RNNs with TensorFlow/Keras:\n",
    "\n",
    "    Meaning and Usage: TensorFlow/Keras provides APIs for building RNNs with ease. You can use recurrent layers such as SimpleRNN, LSTM, or GRU to create RNN architectures for sequence prediction tasks.\n",
    "    Importance: Building RNNs with TensorFlow/Keras allows you to model sequential dependencies in data and make predictions based on context.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleRNN, Dense\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize sequential model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Initialize sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add SimpleRNN layer\n",
    "model.add(SimpleRNN(units=64, activation='relu', input_shape=(sequence_length, input_dim)))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training RNNs and Long Short-Term Memory (LSTM):\n",
    "\n",
    "    Meaning and Usage: Training RNNs involves feeding sequential data into the model and optimizing its parameters using backpropagation through time (BPTT). Long Short-Term Memory (LSTM) is a type of RNN architecture designed to overcome the vanishing gradient problem and capture long-range dependencies.\n",
    "    Importance: LSTM networks are well-suited for tasks that require modeling long-range dependencies, such as language modeling and time series prediction.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Initialize sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "model.add(LSTM(units=128, activation='tanh', return_sequences=True, input_shape=(sequence_length, input_dim)))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Introduction to Transfer Learning:\n",
    "\n",
    "    Meaning and Usage: Transfer learning is a machine learning technique where a model trained on one task is reused as a starting point for a different but related task. Instead of training a model from scratch, transfer learning leverages knowledge learned from a source domain to improve performance on a target domain.\n",
    "    Importance: Transfer learning enables the efficient use of pre-trained models and facilitates training on small datasets or specific tasks by transferring knowledge from large, general datasets.\n",
    "    Key Concepts: Source task, target task, fine-tuning, feature extraction, domain adaptation.\n",
    "\n",
    "Pre-trained Models and Transfer Learning in Deep Learning:\n",
    "\n",
    "    Meaning and Usage: Pre-trained models are deep learning models that have been trained on large-scale datasets, typically for image classification tasks. These models, such as VGG, ResNet, Inception, and BERT, have learned rich representations of the input data and can be used as feature extractors or fine-tuned for specific tasks via transfer learning.\n",
    "    Importance: Pre-trained models offer a powerful starting point for various downstream tasks, allowing practitioners to benefit from the expertise and computational resources required for training large models.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load pre-trained VGG16 model (without top layers)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, img_channels))\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add top layers for classification\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Fine-tune the model on new data\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
